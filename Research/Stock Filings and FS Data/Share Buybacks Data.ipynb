{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pip\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from sqlalchemy import create_engine\n",
    "import re\n",
    "import requests\n",
    "import urllib\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import sys\n",
    "from operator import itemgetter \n",
    "from selenium.webdriver import ActionChains\n",
    "    \n",
    "try:\n",
    "    from selenium import webdriver\n",
    "except ImportError:\n",
    "    pip.main(['install', 'selenium'])\n",
    "    from selenium import webdriver\n",
    "\n",
    "from IPython.display import clear_output\n",
    "pd.options.display.float_format = '{:,.0f}'.format\n",
    "\n",
    "# Progress bar function unrelated to web scraping\n",
    "def update_progress(progress, desc_string):\n",
    "    bar_length = 60\n",
    "    \n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "\n",
    "    block = int(round(bar_length * progress))\n",
    "\n",
    "    # clear()\n",
    "    clear_output(wait = True)\n",
    "    text = \"Progress: [{0}] {1:.1f}% Last Post: {2}\".format( \"#\" * block + \"-\" * (bar_length - block), \n",
    "                                                                   progress * 100,\n",
    "                                                                   desc_string)\n",
    "    print(text)\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "prefs = {\"download.default_directory\" : r\"C:\\Users\\Fang\\Desktop\\Python Trading\\Trading\\Data\\Stock Universe\"}\n",
    "options.add_experimental_option(\"prefs\",prefs)\n",
    "\n",
    "\n",
    "\n",
    "db_dir = r'C:\\Users\\Fang\\Desktop\\Python Trading\\Trading\\Data\\DBs'\n",
    "chrome_dir = r'C:\\Users\\Fang\\Desktop\\Python Trading\\Trading\\Data\\Stock Universe'\n",
    "universe_dir = r'C:\\Users\\Fang\\Desktop\\Python Trading\\Trading\\Data\\Stock Universe'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Universe of Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for logging into EODDATA\n",
    "def eod_index_login(eod_index_user, eod_index_pwd, browser):\n",
    "    \n",
    "    os.chdir(chrome_dir)\n",
    "    \n",
    "    index_data_base_url = 'http://eoddata.com/'\n",
    "    \n",
    "    browser.get(index_data_base_url)\n",
    "    browser.refresh()\n",
    "    browser.delete_all_cookies()\n",
    "    \n",
    "    def login():\n",
    "        email_input = WebDriverWait(browser, 10).until(EC.element_to_be_clickable((By.XPATH, '//input[@name=\"ctl00$cph1$lg1$txtEmail\"]')))\n",
    "        pwd_input = WebDriverWait(browser, 10).until(EC.element_to_be_clickable((By.XPATH, '//input[@name=\"ctl00$cph1$lg1$txtPassword\"]')))\n",
    "        submit_login = WebDriverWait(browser, 10).until(EC.element_to_be_clickable((By.XPATH, '//input[@value=\"Login\"]')))\n",
    "\n",
    "        email_input.click()\n",
    "        email_input.send_keys(Keys.CONTROL, 'a')\n",
    "        email_input.send_keys(eod_index_user)\n",
    "\n",
    "        pwd_input.click()\n",
    "        pwd_input.send_keys(Keys.CONTROL, 'a')\n",
    "        pwd_input.send_keys(eod_index_pwd)\n",
    "\n",
    "        submit_login.click()\n",
    "        \n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        login()\n",
    "    except:\n",
    "        browser.get(index_data_base_url)\n",
    "        browser.refresh()\n",
    "        browser.delete_all_cookies()\n",
    "        \n",
    "        login()\n",
    "\n",
    "# Function for getting latest EODDATA constituents for AMEX, NYSE, and NASDAQ and writing to index.db\n",
    "def update_index_constituents(browser, index_db_name):\n",
    "\n",
    "    sites = {'AMEX':'http://eoddata.com/stocklist/AMEX.htm',\n",
    "             'NYSE':'http://eoddata.com/stocklist/NYSE.htm',\n",
    "             'NASDAQ':'http://eoddata.com/stocklist/NASDAQ.htm'}\n",
    "\n",
    "    for idx, site in sites.items():\n",
    "        browser.get(site)\n",
    "        constituents = WebDriverWait(browser, 10).until(EC.element_to_be_clickable((By.XPATH, '//div[@class=\"hlink\"]')))\n",
    "        constituents.click()\n",
    "    \n",
    "    os.chdir(universe_dir)\n",
    "    for index_txt in list(filter(lambda x: 'txt' == x.split('.')[-1], os.listdir())):\n",
    "        os.chdir(universe_dir)\n",
    "        curr_index_cons = pd.read_table(index_txt)\n",
    "        idx = index_txt.strip('.txt')\n",
    "        curr_index_cons['Index'] = idx\n",
    "        \n",
    "        os.chdir(db_dir)\n",
    "        index_engine = create_engine('sqlite:///{}.db'.format(index_db_name), echo=False)\n",
    "\n",
    "        try:\n",
    "            prior_index_cons = pd.read_sql('SELECT * FROM tickers',\n",
    "                                           con = index_engine)\n",
    "        except:\n",
    "            prior_index_cons = pd.DataFrame()\n",
    "\n",
    "\n",
    "        prior_index_cons.append(curr_index_cons).drop_duplicates().to_sql('tickers',\n",
    "                                                                          con = index_engine,\n",
    "                                                                          index = False,\n",
    "                                                                          if_exists = 'replace')\n",
    "    \n",
    "    os.chdir(universe_dir)\n",
    "    for index_txt in list(filter(lambda x: 'txt' == x.split('.')[-1], os.listdir())):\n",
    "        os.remove(index_txt)\n",
    "    return\n",
    "\n",
    "# YCharts Buyback Data Pull\n",
    "def get_ychart_data(browser, ticker, info_field):\n",
    "    \n",
    "    ychart_link_dict = {'BuyBacks':'https://ycharts.com/companies/{ticker}/stock_buyback'.format(ticker = ticker),\n",
    "                        'GrossMargin':'https://ycharts.com/companies/{ticker}/gross_profit_margin'.format(ticker = ticker),\n",
    "                        'NetMargin':'https://ycharts.com/companies/{ticker}/profit_margin'.format(ticker = ticker),\n",
    "                        'SharesOutstanding':'https://ycharts.com/companies/{ticker}/shares_outstanding'.format(ticker = ticker),\n",
    "                        'DebtToEquity':'https://ycharts.com/companies/{ticker}/debt_equity_ratio'.format(ticker = ticker),\n",
    "                        'CashOnHand':'https://ycharts.com/companies/{ticker}/cash_on_hand'.format(ticker = ticker),\n",
    "                        'MktCap':'https://ycharts.com/companies/{ticker}/market_cap'.format(ticker = ticker),\n",
    "                        'EnterpriseVal':'https://ycharts.com/companies/{ticker}/enterprise_value'.format(ticker = ticker),\n",
    "                        'FCF':'https://ycharts.com/companies/{ticker}/free_cash_flow'.format(ticker = ticker)}\n",
    "\n",
    "    browser.get(ychart_link_dict[info_field])\n",
    "    browser.delete_all_cookies()\n",
    "\n",
    "    ychart_data = WebDriverWait(browser, 3).until(EC.presence_of_element_located((By.XPATH, '//div[@id=\"dataTableBox\"]')))\n",
    "    ychart_data = ychart_data.find_elements_by_tag_name('table')\n",
    "\n",
    "    if type(ychart_data) == list:\n",
    "\n",
    "        ychart_df = pd.DataFrame()\n",
    "        for table in ychart_data:\n",
    "            curr_df = pd.read_html(table.get_attribute('outerHTML'))[0].iloc[1:,:]\n",
    "            curr_df.columns = ['Qtr',info_field]\n",
    "            curr_df.Qtr = pd.to_datetime(curr_df.Qtr)\n",
    "\n",
    "            ychart_df = ychart_df.append(curr_df).reset_index(drop = True)\n",
    "\n",
    "        ychart_df['Ticker'] = ticker\n",
    "        \n",
    "    return ychart_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome(executable_path = chrome_dir + \"\\\\chromedriver.exe\", options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eod_index_email = 'tunihamupi@idx4.com'\n",
    "eod_index_user = 'nahh2018'\n",
    "eod_index_pwd = 'data.2018'\n",
    "index_db_name = 'index'\n",
    "buyback_db_name = 'buybacks'\n",
    "\n",
    "eod_index_login(eod_index_user, eod_index_pwd, browser)\n",
    "update_index_constituents(browser, index_db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Share Buyback Data From YCharts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(db_dir)\n",
    "\n",
    "index_engine = create_engine('sqlite:///{}.db'.format(index_db_name), echo=False)\n",
    "tickers = pd.read_sql('SELECT DISTINCT Symbol FROM tickers',\n",
    "                      con = index_engine)\n",
    "\n",
    "key_stats = ['BuyBacks', 'GrossMargin',\n",
    "             'NetMargin',\n",
    "             'SharesOutstanding',\n",
    "             'DebtToEquity',\n",
    "             'CashOnHand', 'MktCap','EnterpriseVal', 'FCF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [#-----------------------------------------------------------] 2.0% Last Post: 619.08 Seconds CCOR No Data \n"
     ]
    }
   ],
   "source": [
    "buyback_engine = create_engine('sqlite:///{}.db'.format(buyback_db_name), echo=False)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for idx, row in tickers.iloc[55:,:].iterrows():\n",
    "    ticker = row.Symbol\n",
    "    \n",
    "    ychart_url = 'https://ycharts.com/companies/{ticker}/stock_buyback'.format(ticker = ticker)\n",
    "    \n",
    "    try:\n",
    "        browser.refresh()\n",
    "    except:\n",
    "        browser = webdriver.Chrome(executable_path = chrome_dir + \"\\\\chromedriver.exe\", options=options)\n",
    "        \n",
    "    browser.get(ychart_url)\n",
    "    browser.delete_all_cookies()\n",
    "    \n",
    "    try:\n",
    "        page_not_found = WebDriverWait(browser, 1).until(EC.presence_of_element_located((By.XPATH, '//h1[text()=\"Page Not Found\"]')))\n",
    "        \n",
    "        run_time = round(time.time() - start_time, 2)\n",
    "        update_progress(idx/len(tickers.Symbol), '{0} Seconds {1} '.format(run_time, ticker + ' No Data'))\n",
    "        \n",
    "        pd.DataFrame([ticker], columns = ['NoData']).to_sql('nodata',\n",
    "                                                            con = buyback_engine,\n",
    "                                                            index = False,\n",
    "                                                            if_exists = 'append')\n",
    "        continue\n",
    "    except:\n",
    "        \n",
    "        for stat in key_stats:\n",
    "            try:\n",
    "                curr_df = get_ychart_data(browser, ticker, stat)\n",
    "            except:\n",
    "                browser = webdriver.Chrome(executable_path = chrome_dir + \"\\\\chromedriver.exe\", options=options)\n",
    "                time.sleep(2)\n",
    "                curr_df = get_ychart_data(browser, ticker, stat) \n",
    "            curr_df.to_sql(stat,\n",
    "                           con = buyback_engine,\n",
    "                           index = False,\n",
    "                           if_exists = 'append')\n",
    "        \n",
    "    run_time = round(time.time() - start_time, 2)\n",
    "    update_progress(idx/len(tickers.Symbol), '{0} Seconds {1} '.format(run_time, ticker))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
